{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dataset-overview",
      "metadata": {
        "id": "dataset-overview"
      },
      "source": [
        "# Text Simplification Dataset Analysis\n",
        "\n",
        "This notebook analyzes a text simplification dataset with three complexity levels:\n",
        "- **Elementary**: Simplified text for basic readers\n",
        "- **Intermediate**: Medium complexity text\n",
        "- **Advanced**: Complex/original text\n",
        "\n",
        "The dataset contains parallel texts at different complexity levels, making it ideal for text simplification research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeda6530-e788-4ff8-8aef-ffd33dd9a20b",
      "metadata": {
        "id": "aeda6530-e788-4ff8-8aef-ffd33dd9a20b"
      },
      "outputs": [],
      "source": [
        "!pip install textstat\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "from textstat import flesch_reading_ease, flesch_kincaid_grade, automated_readability_index\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import ast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Any\n",
        "from dataclasses import dataclass, field\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from datasets import Dataset as HFDataset, DatasetDict\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\" Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "Y3vd7J_sg4t8"
      },
      "id": "Y3vd7J_sg4t8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "data-loading",
      "metadata": {
        "id": "data-loading"
      },
      "source": [
        "## 1. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "load-data",
      "metadata": {
        "id": "load-data"
      },
      "outputs": [],
      "source": [
        "# Load the combined dataset\n",
        "df = pd.read_csv('all_data.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "data-info",
      "metadata": {
        "id": "data-info"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b78b302-955e-403b-b8d3-9801bae6f122",
      "metadata": {
        "id": "2b78b302-955e-403b-b8d3-9801bae6f122"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "data-preprocessing",
      "metadata": {
        "id": "data-preprocessing"
      },
      "source": [
        "## 2. Data Preprocessing and Text Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "parse-text-data",
      "metadata": {
        "id": "parse-text-data"
      },
      "outputs": [],
      "source": [
        "def safe_eval(text):\n",
        "    \"\"\"Safely evaluate string representation of lists\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    try:\n",
        "        # Try to evaluate as Python literal\n",
        "        return ast.literal_eval(text)\n",
        "    except (ValueError, SyntaxError):\n",
        "        # If that fails, treat as a single string\n",
        "        return [str(text)]\n",
        "\n",
        "# Parse the text columns (they appear to be stored as string representations of lists)\n",
        "df_parsed = df.copy()\n",
        "for col in ['Elementary', 'Intermediate', 'Advanced']:\n",
        "    df_parsed[col] = df_parsed[col].apply(safe_eval)\n",
        "\n",
        "print(\"Sample parsed data:\")\n",
        "for i, row in df_parsed.head(2).iterrows():\n",
        "    print(f\"\\n--- Article {i+1} ---\")\n",
        "    print(f\"Elementary sentences: {len(row['Elementary'])}\")\n",
        "    print(f\"Intermediate sentences: {len(row['Intermediate'])}\")\n",
        "    print(f\"Advanced sentences: {len(row['Advanced'])}\")\n",
        "\n",
        "    if row['Elementary']:\n",
        "        print(f\"\\nFirst Elementary sentence: {row['Elementary'][0][:100]}...\")\n",
        "    if row['Advanced']:\n",
        "        print(f\"First Advanced sentence: {row['Advanced'][0][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "create-sentence-pairs",
      "metadata": {
        "id": "create-sentence-pairs"
      },
      "outputs": [],
      "source": [
        "# Create a flattened dataset with sentence pairs\n",
        "sentence_data = []\n",
        "\n",
        "for idx, row in df_parsed.iterrows():\n",
        "    elementary = row['Elementary']\n",
        "    intermediate = row['Intermediate']\n",
        "    advanced = row['Advanced']\n",
        "\n",
        "    # Get the maximum length to align sentences\n",
        "    max_len = max(len(elementary), len(intermediate), len(advanced))\n",
        "\n",
        "    for i in range(max_len):\n",
        "        elem_sent = elementary[i] if i < len(elementary) else None\n",
        "        inter_sent = intermediate[i] if i < len(intermediate) else None\n",
        "        adv_sent = advanced[i] if i < len(advanced) else None\n",
        "\n",
        "        # Only add if at least elementary and advanced exist\n",
        "        if elem_sent and adv_sent:\n",
        "            sentence_data.append({\n",
        "                'article_id': idx,\n",
        "                'sentence_id': i,\n",
        "                'elementary': elem_sent,\n",
        "                'intermediate': inter_sent,\n",
        "                'advanced': adv_sent\n",
        "            })\n",
        "\n",
        "# Create DataFrame with sentence pairs\n",
        "sentences_df = pd.DataFrame(sentence_data)\n",
        "print(f\"Total sentence pairs: {len(sentences_df)}\")\n",
        "sentences_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "text-analysis",
      "metadata": {
        "id": "text-analysis"
      },
      "source": [
        "## 3. Text Complexity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "download-nltk-data",
      "metadata": {
        "id": "download-nltk-data"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "    nltk.download('stopwords')\n",
        "except Exception as e:\n",
        "    print(f\"Warning: NLTK download failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "readability-metrics",
      "metadata": {
        "id": "readability-metrics"
      },
      "outputs": [],
      "source": [
        "def simple_sentence_tokenize(text):\n",
        "    \"\"\"Simple sentence tokenizer that doesn't rely on NLTK\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    # Split on sentence ending punctuation\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    # Remove empty strings and strip whitespace\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "def simple_word_tokenize(text):\n",
        "    \"\"\"Simple word tokenizer that doesn't rely on NLTK\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    # Split on whitespace and punctuation, keep only alphabetic words\n",
        "    words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
        "    return words\n",
        "\n",
        "def safe_textstat_score(text, metric_func):\n",
        "    \"\"\"Safely calculate textstat metrics with error handling\"\"\"\n",
        "    try:\n",
        "        if not text or pd.isna(text) or len(text.strip()) == 0:\n",
        "            return 0\n",
        "        # Some textstat functions fail on very short texts\n",
        "        if len(text.strip()) < 10:\n",
        "            return 0\n",
        "        return metric_func(text)\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def calculate_text_metrics(text):\n",
        "    \"\"\"Calculate various text complexity metrics with robust error handling\"\"\"\n",
        "\n",
        "    # Handle empty or NaN input\n",
        "    if not text or pd.isna(text):\n",
        "        return {\n",
        "            'word_count': 0,\n",
        "            'sentence_count': 0,\n",
        "            'avg_word_length': 0,\n",
        "            'flesch_score': 0,\n",
        "            'flesch_grade': 0,\n",
        "            'automated_readability': 0\n",
        "        }\n",
        "\n",
        "    # Convert to string if it's not already\n",
        "    text = str(text)\n",
        "\n",
        "    # Basic tokenization without NLTK dependencies\n",
        "    try:\n",
        "        words = simple_word_tokenize(text)\n",
        "        sentences = simple_sentence_tokenize(text)\n",
        "    except:\n",
        "        # Fallback: very simple tokenization\n",
        "        words = text.lower().split()\n",
        "        words = [re.sub(r'[^a-zA-Z]', '', word) for word in words]\n",
        "        words = [word for word in words if word]\n",
        "        sentences = text.split('.')\n",
        "        sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "    # Calculate basic metrics\n",
        "    word_count = len(words)\n",
        "    sentence_count = len(sentences) if sentences else 1  # Avoid division by zero\n",
        "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
        "\n",
        "    # Calculate readability scores with error handling\n",
        "    try:\n",
        "        flesch_score = safe_textstat_score(text, flesch_reading_ease)\n",
        "        flesch_grade = safe_textstat_score(text, flesch_kincaid_grade)\n",
        "        automated_readability = safe_textstat_score(text, automated_readability_index)\n",
        "    except ImportError:\n",
        "        # If textstat is not available, use simple approximations\n",
        "        if sentence_count > 0 and word_count > 0:\n",
        "            avg_sentence_length = word_count / sentence_count\n",
        "            # Simple syllable approximation: count vowel groups\n",
        "            total_syllables = sum([max(1, len(re.findall(r'[aeiouAEIOU]', word))) for word in words])\n",
        "            avg_syllables_per_word = total_syllables / word_count if word_count > 0 else 0\n",
        "\n",
        "            flesch_score = max(0, min(100, 206.835 - 1.015 * avg_sentence_length - 84.6 * avg_syllables_per_word))\n",
        "            flesch_grade = max(0, 0.39 * avg_sentence_length + 11.8 * avg_syllables_per_word - 15.59)\n",
        "            automated_readability = max(0, 4.71 * avg_syllables_per_word + 0.5 * avg_sentence_length - 21.43)\n",
        "        else:\n",
        "            flesch_score = 0\n",
        "            flesch_grade = 0\n",
        "            automated_readability = 0\n",
        "    except Exception as e:\n",
        "        # If any other error occurs, set to 0\n",
        "        flesch_score = 0\n",
        "        flesch_grade = 0\n",
        "        automated_readability = 0\n",
        "\n",
        "    return {\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count,\n",
        "        'avg_word_length': round(avg_word_length, 2),\n",
        "        'flesch_score': round(flesch_score, 2),\n",
        "        'flesch_grade': round(flesch_grade, 2),\n",
        "        'automated_readability': round(automated_readability, 2)\n",
        "    }\n",
        "\n",
        "# Calculate metrics for each complexity level\n",
        "print(\"Calculating text complexity metrics...\")\n",
        "for level in ['elementary', 'intermediate', 'advanced']:\n",
        "    print(f\"Processing {level} level...\")\n",
        "    try:\n",
        "        metrics = sentences_df[level].apply(calculate_text_metrics)\n",
        "\n",
        "        # Convert to DataFrame and add to main dataframe\n",
        "        metrics_df = pd.DataFrame(metrics.tolist())\n",
        "        for col in metrics_df.columns:\n",
        "            sentences_df[f'{level}_{col}'] = metrics_df[col]\n",
        "        print(f\"  ‚úì {level} metrics calculated successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Error calculating {level} metrics: {e}\")\n",
        "        # Add empty columns if calculation fails\n",
        "        for metric in ['word_count', 'sentence_count', 'avg_word_length', 'flesch_score', 'flesch_grade', 'automated_readability']:\n",
        "            sentences_df[f'{level}_{metric}'] = 0\n",
        "\n",
        "print(\"\\nMetrics calculation complete!\")\n",
        "print(f\"Dataset now has {sentences_df.shape[1]} columns\")\n",
        "sentences_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "complexity-comparison",
      "metadata": {
        "id": "complexity-comparison"
      },
      "outputs": [],
      "source": [
        "# Compare complexity metrics across levels\n",
        "metrics_to_compare = ['word_count', 'avg_word_length', 'flesch_score', 'flesch_grade']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, metric in enumerate(metrics_to_compare):\n",
        "    data_to_plot = []\n",
        "    labels = []\n",
        "\n",
        "    for level in ['elementary', 'intermediate', 'advanced']:\n",
        "        col_name = f'{level}_{metric}'\n",
        "        if col_name in sentences_df.columns:\n",
        "            data_to_plot.append(sentences_df[col_name].dropna())\n",
        "            labels.append(level.capitalize())\n",
        "\n",
        "    if data_to_plot:\n",
        "        axes[i].boxplot(data_to_plot, labels=labels)\n",
        "        axes[i].set_title(f'{metric.replace(\"_\", \" \").title()} by Complexity Level')\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\nSummary Statistics by Complexity Level:\")\n",
        "for metric in metrics_to_compare:\n",
        "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
        "    for level in ['elementary', 'intermediate', 'advanced']:\n",
        "        col_name = f'{level}_{metric}'\n",
        "        if col_name in sentences_df.columns:\n",
        "            mean_val = sentences_df[col_name].mean()\n",
        "            std_val = sentences_df[col_name].std()\n",
        "            print(f\"  {level.capitalize()}: {mean_val:.2f} (¬±{std_val:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simplification-examples",
      "metadata": {
        "id": "simplification-examples"
      },
      "source": [
        "## 4. Text Simplification Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "show-examples",
      "metadata": {
        "id": "show-examples"
      },
      "outputs": [],
      "source": [
        "# Show examples of text simplification\n",
        "def show_simplification_examples(df, n=5):\n",
        "    \"\"\"Display examples of text at different complexity levels\"\"\"\n",
        "\n",
        "    # Filter out rows with missing data\n",
        "    complete_rows = df.dropna(subset=['elementary', 'advanced'])\n",
        "\n",
        "    # Select random examples\n",
        "    examples = complete_rows.sample(n=min(n, len(complete_rows)))\n",
        "\n",
        "    for i, (idx, row) in enumerate(examples.iterrows()):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"EXAMPLE {i+1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        print(\"\\nüî¥ ADVANCED (Original):\")\n",
        "        print(f\"{row['advanced']}\")\n",
        "\n",
        "        if pd.notna(row['intermediate']):\n",
        "            print(\"\\nüü° INTERMEDIATE:\")\n",
        "            print(f\"{row['intermediate']}\")\n",
        "\n",
        "        print(\"\\nüü¢ ELEMENTARY (Simplified):\")\n",
        "        print(f\"{row['elementary']}\")\n",
        "\n",
        "        # Show metrics\n",
        "        print(\"\\nüìä COMPLEXITY METRICS:\")\n",
        "        for level in ['advanced', 'elementary']:\n",
        "            if f'{level}_word_count' in df.columns:\n",
        "                wc = row[f'{level}_word_count']\n",
        "                awl = row[f'{level}_avg_word_length']\n",
        "                flesch = row[f'{level}_flesch_score']\n",
        "                print(f\"  {level.upper()}: {wc} words, {awl:.1f} avg word length, {flesch:.1f} Flesch score\")\n",
        "\n",
        "show_simplification_examples(sentences_df, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocabulary-analysis",
      "metadata": {
        "id": "vocabulary-analysis"
      },
      "source": [
        "## 5. Vocabulary Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocabulary-stats",
      "metadata": {
        "id": "vocabulary-stats"
      },
      "outputs": [],
      "source": [
        "def analyze_vocabulary(texts, level_name):\n",
        "    \"\"\"Analyze vocabulary usage in texts\"\"\"\n",
        "    all_words = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for text in texts:\n",
        "        if pd.notna(text):\n",
        "            words = word_tokenize(text.lower())\n",
        "            # Filter out punctuation and stop words\n",
        "            words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "            all_words.extend(words)\n",
        "\n",
        "    word_freq = Counter(all_words)\n",
        "    unique_words = len(word_freq)\n",
        "    total_words = len(all_words)\n",
        "\n",
        "    print(f\"\\n{level_name.upper()} VOCABULARY:\")\n",
        "    print(f\"  Total words: {total_words:,}\")\n",
        "    print(f\"  Unique words: {unique_words:,}\")\n",
        "    print(f\"  Vocabulary richness: {unique_words/total_words:.3f}\")\n",
        "    print(f\"  Top 10 words: {word_freq.most_common(10)}\")\n",
        "\n",
        "    return word_freq\n",
        "\n",
        "# Analyze vocabulary for each level\n",
        "vocab_stats = {}\n",
        "for level in ['elementary', 'intermediate', 'advanced']:\n",
        "    vocab_stats[level] = analyze_vocabulary(sentences_df[level], level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocabulary-comparison",
      "metadata": {
        "id": "vocabulary-comparison"
      },
      "outputs": [],
      "source": [
        "# Compare vocabulary overlap between levels\n",
        "def vocabulary_overlap(vocab1, vocab2, name1, name2):\n",
        "    \"\"\"Calculate vocabulary overlap between two levels\"\"\"\n",
        "    words1 = set(vocab1.keys())\n",
        "    words2 = set(vocab2.keys())\n",
        "\n",
        "    overlap = words1.intersection(words2)\n",
        "    only_in_1 = words1 - words2\n",
        "    only_in_2 = words2 - words1\n",
        "\n",
        "    print(f\"\\nVOCABULARY OVERLAP: {name1.upper()} vs {name2.upper()}\")\n",
        "    print(f\"  Shared words: {len(overlap)} ({len(overlap)/len(words1.union(words2)):.1%})\")\n",
        "    print(f\"  Only in {name1}: {len(only_in_1)}\")\n",
        "    print(f\"  Only in {name2}: {len(only_in_2)}\")\n",
        "\n",
        "    return overlap, only_in_1, only_in_2\n",
        "\n",
        "# Compare elementary vs advanced\n",
        "if 'elementary' in vocab_stats and 'advanced' in vocab_stats:\n",
        "    overlap_ea, only_elem, only_adv = vocabulary_overlap(\n",
        "        vocab_stats['elementary'],\n",
        "        vocab_stats['advanced'],\n",
        "        'elementary',\n",
        "        'advanced'\n",
        "    )\n",
        "\n",
        "    print(f\"\\nWords unique to ADVANCED (sample): {list(only_adv)[:20]}\")\n",
        "    print(f\"Words unique to ELEMENTARY (sample): {list(only_elem)[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-preparation",
      "metadata": {
        "id": "dataset-preparation"
      },
      "source": [
        "## 6. Dataset Preparation for Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prepare-ml-dataset",
      "metadata": {
        "id": "prepare-ml-dataset"
      },
      "outputs": [],
      "source": [
        "# Prepare datasets for different simplification tasks\n",
        "\n",
        "# 1. Advanced to Elementary simplification\n",
        "adv_to_elem = sentences_df[['advanced', 'elementary']].dropna()\n",
        "adv_to_elem.columns = ['source', 'target']\n",
        "adv_to_elem['task'] = 'advanced_to_elementary'\n",
        "\n",
        "# 2. Advanced to Intermediate simplification\n",
        "adv_to_inter = sentences_df[['advanced', 'intermediate']].dropna()\n",
        "adv_to_inter.columns = ['source', 'target']\n",
        "adv_to_inter['task'] = 'advanced_to_intermediate'\n",
        "\n",
        "# 3. Intermediate to Elementary simplification\n",
        "inter_to_elem = sentences_df[['intermediate', 'elementary']].dropna()\n",
        "inter_to_elem.columns = ['source', 'target']\n",
        "inter_to_elem['task'] = 'intermediate_to_elementary'\n",
        "\n",
        "# Combine all tasks\n",
        "ml_dataset = pd.concat([adv_to_elem, adv_to_inter, inter_to_elem], ignore_index=True)\n",
        "\n",
        "print(f\"Machine Learning Dataset Summary:\")\n",
        "print(f\"Total pairs: {len(ml_dataset)}\")\n",
        "print(f\"\\nBy task:\")\n",
        "print(ml_dataset['task'].value_counts())\n",
        "\n",
        "# Save the ML dataset\n",
        "ml_dataset.to_csv('text_simplification_ml_dataset.csv', index=False)\n",
        "print(f\"\\nDataset saved as 'text_simplification_ml_dataset.csv'\")\n",
        "\n",
        "ml_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train-test-split",
      "metadata": {
        "id": "train-test-split"
      },
      "outputs": [],
      "source": [
        "# Create train/validation/test splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_splits(df, test_size=0.2, val_size=0.1, random_state=42):\n",
        "    \"\"\"Create train/validation/test splits\"\"\"\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_val, test = train_test_split(\n",
        "        df, test_size=test_size, random_state=random_state, stratify=df['task']\n",
        "    )\n",
        "\n",
        "    # Second split: separate validation from training\n",
        "    val_size_adjusted = val_size / (1 - test_size)  # Adjust validation size\n",
        "    train, val = train_test_split(\n",
        "        train_val, test_size=val_size_adjusted, random_state=random_state, stratify=train_val['task']\n",
        "    )\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "# Create splits for each task\n",
        "splits = {}\n",
        "\n",
        "for task in ml_dataset['task'].unique():\n",
        "    task_data = ml_dataset[ml_dataset['task'] == task].copy()\n",
        "\n",
        "    if len(task_data) >= 10:  # Only split if we have enough data\n",
        "        # For small datasets, just do train/test split\n",
        "        train_data, test_data = train_test_split(\n",
        "            task_data, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Use 10% of training data for validation\n",
        "        if len(train_data) >= 10:\n",
        "            train_data, val_data = train_test_split(\n",
        "                train_data, test_size=0.125, random_state=42  # 0.125 of 0.8 = 0.1 of total\n",
        "            )\n",
        "        else:\n",
        "            val_data = train_data.sample(n=min(2, len(train_data)//2), random_state=42)\n",
        "            train_data = train_data.drop(val_data.index)\n",
        "\n",
        "        splits[task] = {\n",
        "            'train': train_data,\n",
        "            'val': val_data,\n",
        "            'test': test_data\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{task.upper()} SPLITS:\")\n",
        "        print(f\"  Train: {len(train_data)} samples\")\n",
        "        print(f\"  Validation: {len(val_data)} samples\")\n",
        "        print(f\"  Test: {len(test_data)} samples\")\n",
        "\n",
        "        # Save splits\n",
        "        train_data.to_csv(f'{task}_train.csv', index=False)\n",
        "        val_data.to_csv(f'{task}_val.csv', index=False)\n",
        "        test_data.to_csv(f'{task}_test.csv', index=False)\n",
        "\n",
        "print(\"\\nDataset splits saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-summary",
      "metadata": {
        "id": "final-summary"
      },
      "outputs": [],
      "source": [
        "# Final dataset summary\n",
        "print(\"üìä FINAL DATASET SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìÑ Total articles: {len(df_parsed)}\")\n",
        "print(f\"üìù Total sentence pairs: {len(sentences_df)}\")\n",
        "print(f\"ü§ñ ML dataset pairs: {len(ml_dataset)}\")\n",
        "\n",
        "print(f\"\\nüìà COMPLEXITY ANALYSIS:\")\n",
        "for level in ['elementary', 'intermediate', 'advanced']:\n",
        "    if f'{level}_flesch_score' in sentences_df.columns:\n",
        "        avg_flesch = sentences_df[f'{level}_flesch_score'].mean()\n",
        "        avg_words = sentences_df[f'{level}_word_count'].mean()\n",
        "        print(f\"  {level.capitalize()}: {avg_flesch:.1f} Flesch score, {avg_words:.1f} avg words\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c235857-b354-41cb-bd89-f8deaf9be3e9",
      "metadata": {
        "id": "7c235857-b354-41cb-bd89-f8deaf9be3e9"
      },
      "outputs": [],
      "source": [
        "\n",
        "original_count = len(ml_dataset)\n",
        "print(f\"Original dataset: {original_count} pairs\")\n",
        "\n",
        "# Remove empty/very short texts\n",
        "ml_dataset_clean = ml_dataset.dropna(subset=['source', 'target'])\n",
        "ml_dataset_clean = ml_dataset_clean[\n",
        "    (ml_dataset_clean['source'].str.len() > 10) &\n",
        "    (ml_dataset_clean['target'].str.len() > 10)\n",
        "]\n",
        "\n",
        "# Remove identical source-target pairs (wastes training time)\n",
        "ml_dataset_clean = ml_dataset_clean[ml_dataset_clean['source'] != ml_dataset_clean['target']]\n",
        "\n",
        "# Remove extremely long texts (causes memory issues)\n",
        "ml_dataset_clean = ml_dataset_clean[\n",
        "    (ml_dataset_clean['source'].str.len() < 1000) &\n",
        "    (ml_dataset_clean['target'].str.len() < 1000)\n",
        "]\n",
        "\n",
        "print(f\"Filtered dataset: {len(ml_dataset_clean)} pairs\")\n",
        "print(f\"Kept: {len(ml_dataset_clean)/original_count:.1%} of original data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34955b4-f998-4620-97b5-44f53fa067e8",
      "metadata": {
        "id": "a34955b4-f998-4620-97b5-44f53fa067e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# T5 expects: \"simplify: [complex text]\" ‚Üí \"[simple text]\"\n",
        "ml_dataset_clean['input_text'] = 'simplify: ' + ml_dataset_clean['source']\n",
        "ml_dataset_clean['target_text'] = ml_dataset_clean['target']\n",
        "\n",
        "print(\"‚úÖ T5 format created\")\n",
        "print(\"Input format: 'simplify: [complex text]'\")\n",
        "print(\"Target format: '[simplified text]'\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nüìã TRAINING FORMAT EXAMPLES:\")\n",
        "for i, row in ml_dataset_clean.head(3).iterrows():\n",
        "    print(f\"\\nExample {i+1}:\")\n",
        "    print(f\"Input:  {row['input_text'][:100]}...\")\n",
        "    print(f\"Target: {row['target_text'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44db19b2-8a02-429a-a1a6-b09cf2d98b67",
      "metadata": {
        "id": "44db19b2-8a02-429a-a1a6-b09cf2d98b67"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs('training_ready', exist_ok=True)\n",
        "\n",
        "# Split by task to maintain balance\n",
        "all_train, all_val, all_test = [], [], []\n",
        "\n",
        "for task in ml_dataset_clean['task'].unique():\n",
        "    task_data = ml_dataset_clean[ml_dataset_clean['task'] == task].copy()\n",
        "    print(f\"{task}: {len(task_data)} pairs\")\n",
        "\n",
        "    if len(task_data) >= 10:\n",
        "        # 80/10/10 split\n",
        "        train, temp = train_test_split(task_data, test_size=0.2, random_state=42)\n",
        "        val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
        "\n",
        "        all_train.append(train)\n",
        "        all_val.append(val)\n",
        "        all_test.append(test)\n",
        "    else:\n",
        "        # Too few samples, put in training\n",
        "        all_train.append(task_data)\n",
        "\n",
        "# Combine all tasks\n",
        "train_df = pd.concat(all_train, ignore_index=True)\n",
        "val_df = pd.concat(all_val, ignore_index=True) if all_val else train_df.sample(n=min(50, len(train_df)//10))\n",
        "test_df = pd.concat(all_test, ignore_index=True) if all_test else train_df.sample(n=min(50, len(train_df)//10))\n",
        "\n",
        "print(f\"\\nüìä FINAL SPLITS:\")\n",
        "print(f\"Train: {len(train_df)} pairs\")\n",
        "print(f\"Val: {len(val_df)} pairs\")\n",
        "print(f\"Test: {len(test_df)} pairs\")\n",
        "\n",
        "# Save as JSONL (standard format for training)\n",
        "train_df[['input_text', 'target_text']].to_json('training_ready/train.jsonl', orient='records', lines=True)\n",
        "val_df[['input_text', 'target_text']].to_json('training_ready/val.jsonl', orient='records', lines=True)\n",
        "test_df[['input_text', 'target_text']].to_json('training_ready/test.jsonl', orient='records', lines=True)\n",
        "\n",
        "# Also save as CSV for inspection\n",
        "train_df.to_csv('training_ready/train.csv', index=False)\n",
        "val_df.to_csv('training_ready/val.csv', index=False)\n",
        "test_df.to_csv('training_ready/test.csv', index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ TRAINING FILES SAVED!\")\n",
        "print(f\"üìÅ Location: training_ready/\")\n",
        "print(f\"üìÑ Files: train.jsonl, val.jsonl, test.jsonl\")\n",
        "print(f\"üöÄ READY TO TRAIN T5 MODEL!\")\n",
        "\n",
        "print(f\"\\nüéØ NEXT STEPS:\")\n",
        "print(f\"1. Install: pip install transformers datasets torch\")\n",
        "print(f\"2. Use training_ready/train.jsonl for training\")\n",
        "print(f\"3. Use training_ready/val.jsonl for validation\")\n",
        "print(f\"4. Start with t5-small model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d78fff4-23a7-458c-9616-96fde0861747",
      "metadata": {
        "id": "2d78fff4-23a7-458c-9616-96fde0861747"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"Training configuration.\"\"\"\n",
        "    model_name: str = \"t5-small\"\n",
        "    max_input_length: int = 512\n",
        "    max_target_length: int = 256\n",
        "    output_dir: str = \"t5-simplification-model\"\n",
        "\n",
        "    # Training hyperparameters\n",
        "    batch_size: int = 2  # Reduced for stability\n",
        "    num_epochs: int = 3\n",
        "    learning_rate: float = 5e-5\n",
        "    warmup_steps: int = 500\n",
        "    eval_steps: int = 200\n",
        "    save_steps: int = 200\n",
        "\n",
        "class RobustSimplificationDataset(Dataset):\n",
        "    \"\"\"Ultra-robust dataset with strict validation.\"\"\"\n",
        "\n",
        "    def __init__(self, data_file: str, tokenizer, config: ModelConfig):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.config = config\n",
        "        self.data = self._load_and_validate_data(data_file)\n",
        "\n",
        "    def _load_and_validate_data(self, data_file: str):\n",
        "        \"\"\"Load and strictly validate all data.\"\"\"\n",
        "        valid_data = []\n",
        "\n",
        "        with open(data_file, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    item = json.loads(line.strip())\n",
        "\n",
        "                    # Validate required fields\n",
        "                    if not ('input_text' in item and 'target_text' in item):\n",
        "                        continue\n",
        "\n",
        "                    # Clean and validate text\n",
        "                    input_text = str(item['input_text']).strip()\n",
        "                    target_text = str(item['target_text']).strip()\n",
        "\n",
        "                    # Skip empty or very short texts\n",
        "                    if len(input_text) < 10 or len(target_text) < 5:\n",
        "                        continue\n",
        "\n",
        "                    # Test tokenization\n",
        "                    try:\n",
        "                        # Try tokenizing to ensure it works\n",
        "                        input_tokens = self.tokenizer(\n",
        "                            input_text,\n",
        "                            max_length=self.config.max_input_length,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            return_tensors='pt'\n",
        "                        )\n",
        "\n",
        "                        target_tokens = self.tokenizer(\n",
        "                            target_text,\n",
        "                            max_length=self.config.max_target_length,\n",
        "                            padding='max_length',\n",
        "                            truncation=True,\n",
        "                            return_tensors='pt'\n",
        "                        )\n",
        "\n",
        "                        # Verify shapes\n",
        "                        if (input_tokens.input_ids.shape[1] == self.config.max_input_length and\n",
        "                            target_tokens.input_ids.shape[1] == self.config.max_target_length):\n",
        "\n",
        "                            valid_data.append({\n",
        "                                'input_text': input_text,\n",
        "                                'target_text': target_text\n",
        "                            })\n",
        "\n",
        "                    except Exception as token_error:\n",
        "                        print(f\"‚ö†Ô∏è Tokenization failed for sample {i}: {token_error}\")\n",
        "                        continue\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Skipping line {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "        print(f\"üìä Loaded {len(valid_data)} validated examples from {data_file}\")\n",
        "        return valid_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Get item with guaranteed consistent shapes.\"\"\"\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Tokenize input\n",
        "        input_encoding = self.tokenizer(\n",
        "            item['input_text'],\n",
        "            max_length=self.config.max_input_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Tokenize target\n",
        "        target_encoding = self.tokenizer(\n",
        "            item['target_text'],\n",
        "            max_length=self.config.max_target_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Ensure correct shapes and flatten\n",
        "        input_ids = input_encoding.input_ids.squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = input_encoding.attention_mask.squeeze(0)\n",
        "        labels = target_encoding.input_ids.squeeze(0)\n",
        "\n",
        "        # Final shape validation\n",
        "        assert input_ids.shape == torch.Size([self.config.max_input_length])\n",
        "        assert attention_mask.shape == torch.Size([self.config.max_input_length])\n",
        "        assert labels.shape == torch.Size([self.config.max_target_length])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels\n",
        "        }\n",
        "\n",
        "# Recreate datasets with robust class\n",
        "print(\"üîÑ Recreating datasets with robust validation...\")\n",
        "\n",
        "train_dataset = RobustSimplificationDataset(\"training_ready/train.jsonl\", tokenizer, config)\n",
        "val_dataset = RobustSimplificationDataset(\"training_ready/val.jsonl\", tokenizer, config)\n",
        "\n",
        "print(f\"‚úÖ Robust datasets created:\")\n",
        "print(f\"   Train: {len(train_dataset)} samples\")\n",
        "print(f\"   Val: {len(val_dataset)} samples\")\n",
        "\n",
        "# Initialize configuration\n",
        "config = ModelConfig()\n",
        "print(\"‚öôÔ∏è Training Configuration (Fixed):\")\n",
        "print(f\"   Model: {config.model_name}\")\n",
        "print(f\"   Batch size: {config.batch_size} (reduced for stability)\")\n",
        "print(f\"   Epochs: {config.num_epochs}\")\n",
        "print(f\"   Learning rate: {config.learning_rate}\")\n",
        "print(f\"   Output directory: {config.output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "667bd1a2-29b0-41dc-9d24-3133524e4b5c",
      "metadata": {
        "id": "667bd1a2-29b0-41dc-9d24-3133524e4b5c"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred, tokenizer) -> Dict[str, float]:\n",
        "    \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Replace -100 tokens with pad token for decoding\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        scores = scorer.score(label, pred)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    return {\n",
        "        'rouge1': np.mean(rouge1_scores),\n",
        "        'rouge2': np.mean(rouge2_scores),\n",
        "        'rougeL': np.mean(rougeL_scores)\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Evaluation metrics function ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37708627-572f-47ba-965d-35720162e2bb",
      "metadata": {
        "id": "37708627-572f-47ba-965d-35720162e2bb"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "print(\"üì• Loading T5 tokenizer and model...\")\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(config.model_name)\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Model info\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"‚úÖ Model loaded: {config.model_name}\")\n",
        "print(f\"   Parameters: {num_params:,} ({num_params/1e6:.1f}M)\")\n",
        "print(f\"   Device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Test tokenizer\n",
        "test_input = \"simplify: This is a complex sentence that needs simplification.\"\n",
        "test_tokens = tokenizer(test_input, return_tensors=\"pt\")\n",
        "print(f\"\\nüî§ Tokenizer test:\")\n",
        "print(f\"   Input: {test_input}\")\n",
        "print(f\"   Tokens: {test_tokens['input_ids'].shape}\")\n",
        "print(f\"   Decoded: {tokenizer.decode(test_tokens['input_ids'][0], skip_special_tokens=True)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a496ec0-7b7d-482c-b4b8-55173743b27d",
      "metadata": {
        "id": "5a496ec0-7b7d-482c-b4b8-55173743b27d"
      },
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "print(\"üìä Creating training datasets...\")\n",
        "\n",
        "train_dataset = SimplificationDataset(\n",
        "    \"training_ready/train.jsonl\",\n",
        "    tokenizer,\n",
        "    config\n",
        ")\n",
        "\n",
        "val_dataset = SimplificationDataset(\n",
        "    \"training_ready/val.jsonl\",\n",
        "    tokenizer,\n",
        "    config\n",
        ")\n",
        "\n",
        "test_dataset = SimplificationDataset(\n",
        "    \"training_ready/test.jsonl\",\n",
        "    tokenizer,\n",
        "    config\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Dataset sizes:\")\n",
        "print(f\"   Train: {len(train_dataset)} examples\")\n",
        "print(f\"   Val: {len(val_dataset)} examples\")\n",
        "print(f\"   Test: {len(test_dataset)} examples\")\n",
        "\n",
        "# Test dataset loading\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nüîç Sample data shapes:\")\n",
        "print(f\"   Input IDs: {sample['input_ids'].shape}\")\n",
        "print(f\"   Attention mask: {sample['attention_mask'].shape}\")\n",
        "print(f\"   Labels: {sample['labels'].shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bff68e7-0cf6-455c-b413-ba5eb7cb7131",
      "metadata": {
        "id": "5bff68e7-0cf6-455c-b413-ba5eb7cb7131"
      },
      "outputs": [],
      "source": [
        "# Ultra-simple data collator that handles shapes explicitly\n",
        "class UltraSimpleCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features):\n",
        "        \"\"\"Handle batching with explicit shape checking.\"\"\"\n",
        "\n",
        "        # Extract features\n",
        "        input_ids_list = []\n",
        "        attention_mask_list = []\n",
        "        labels_list = []\n",
        "\n",
        "        for feature in features:\n",
        "            input_ids_list.append(feature['input_ids'])\n",
        "            attention_mask_list.append(feature['attention_mask'])\n",
        "            labels_list.append(feature['labels'])\n",
        "\n",
        "        # Stack with shape validation\n",
        "        try:\n",
        "            input_ids = torch.stack(input_ids_list, dim=0)\n",
        "            attention_mask = torch.stack(attention_mask_list, dim=0)\n",
        "            labels = torch.stack(labels_list, dim=0)\n",
        "\n",
        "            # Verify final batch shapes\n",
        "            batch_size = len(features)\n",
        "            expected_input_shape = (batch_size, config.max_input_length)\n",
        "            expected_labels_shape = (batch_size, config.max_target_length)\n",
        "\n",
        "            assert input_ids.shape == expected_input_shape, f\"Input shape mismatch: {input_ids.shape} vs {expected_input_shape}\"\n",
        "            assert labels.shape == expected_labels_shape, f\"Labels shape mismatch: {labels.shape} vs {expected_labels_shape}\"\n",
        "\n",
        "            return {\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': labels\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\" Collation error: {e}\")\n",
        "            print(f\"Feature shapes:\")\n",
        "            for i, feature in enumerate(features):\n",
        "                print(f\"  Feature {i}: input={feature['input_ids'].shape}, mask={feature['attention_mask'].shape}, labels={feature['labels'].shape}\")\n",
        "            raise\n",
        "\n",
        "# Use the ultra-simple collator\n",
        "data_collator = UltraSimpleCollator(tokenizer)\n",
        "print(\" Ultra-simple data collator created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new trainer with all fixes\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics_bound,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Find and resume from checkpoint\n",
        "checkpoints = glob.glob(f\"{config.output_dir}/checkpoint-*\")\n",
        "\n",
        "print(\"üöÄ Starting training with all fixes...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    if checkpoints:\n",
        "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]))\n",
        "        print(f\"Resuming from: {latest_checkpoint}\")\n",
        "        training_result = trainer.train(resume_from_checkpoint=latest_checkpoint)\n",
        "    else:\n",
        "        print(\"Starting fresh training\")\n",
        "        training_result = trainer.train()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nüéâ Training completed successfully!\")\n",
        "    print(f\"Training time: {training_time/60:.1f} minutes\")\n",
        "    print(f\"Final loss: {training_result.training_loss:.4f}\")\n",
        "\n",
        "    # Save the model\n",
        "    trainer.save_model()\n",
        "    tokenizer.save_pretrained(config.output_dir)\n",
        "    print(f\" Model saved to: {config.output_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\" Training failed again: {e}\")\n",
        "\n",
        "    # Emergency: Skip evaluation and just train\n",
        "    print(\" Trying training without evaluation...\")\n",
        "\n",
        "    # Remove evaluation from training args\n",
        "    training_args.eval_strategy = \"no\"\n",
        "    training_args.save_strategy = \"epoch\"\n",
        "\n",
        "    trainer_no_eval = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        training_result = trainer_no_eval.train()\n",
        "        print(\"‚úÖ Training without evaluation successful!\")\n",
        "        trainer_no_eval.save_model()\n",
        "    except Exception as final_error:\n",
        "        print(f\"‚ùå Final attempt failed: {final_error}\")"
      ],
      "metadata": {
        "id": "VAVeMxySkgAi"
      },
      "id": "VAVeMxySkgAi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"üöÄ Starting T5 text simplification training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Train the model\n",
        "    training_result = trainer.train()\n",
        "\n",
        "    # Calculate training time\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üéâ Training completed successfully!\")\n",
        "    print(f\"‚è±Ô∏è Training time: {training_time/60:.1f} minutes\")\n",
        "    print(f\"üìä Final training loss: {training_result.training_loss:.4f}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
        "    print(\"üíæ Saving current state...\")\n",
        "    trainer.save_model()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Training failed with error: {e}\")\n",
        "    print(\"üíæ Attempting to save model state...\")\n",
        "    try:\n",
        "        trainer.save_model()\n",
        "        print(\"‚úÖ Model state saved\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not save model state\")\n"
      ],
      "metadata": {
        "id": "BGYeQ7Fhkiex"
      },
      "id": "BGYeQ7Fhkiex",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}