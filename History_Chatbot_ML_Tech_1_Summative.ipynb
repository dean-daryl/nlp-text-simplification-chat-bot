{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## History Chatbot - Unsloth Fine-Tuning Documentation\n",
        "Complete cell-by-cell documentation for fine-tuning Phi-3-mini on Wikipedia history articles using Unsloth and LoRA.\n",
        "\n",
        "Overview\n",
        "Model: Phi-3-mini (3.8B parameters)\n",
        "Method: LoRA Fine-Tuning\n",
        "Dataset: ~25,000 History Q&A pairs from Wikipedia\n",
        "Training Time: 2-3 hours on T4 GPU\n",
        "Memory Required: 4-5GB VRAM"
      ],
      "metadata": {
        "id": "ra9LJ2ChuWt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the required Python libraries for efficient LLM fine-tuning."
      ],
      "metadata": {
        "id": "FXJ6xIStrjB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth trl peft accelerate bitsandbytes\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "zCKxD3bvrfDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify CUDA availability and displays GPU specifications to ensure proper hardware setup.\n"
      ],
      "metadata": {
        "id": "YPAFExd8roR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"üîß CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ],
      "metadata": {
        "id": "nfGDARIhrnye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create History Q&A Dataset from Wikipedia\n",
        "This Generates conversational Q&A pairs from Wikipedia history articles to create a specialized training dataset."
      ],
      "metadata": {
        "id": "qmu3iUkLruqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dNlnmnwdrhEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üì• Loading Wikipedia dataset...\")\n",
        "# Using WikiAuto which contains Wikipedia content\n",
        "wiki_data = load_dataset(\"GEM/wiki_auto_asset_turk\", split=\"train\")\n",
        "\n",
        "def extract_history_content(text):\n",
        "    \"\"\"Check if content is history-related\"\"\"\n",
        "    history_keywords = [\n",
        "        'war', 'century', 'ancient', 'empire', 'kingdom', 'battle', 'revolution',\n",
        "        'king', 'queen', 'emperor', 'dynasty', 'civilization', 'historical',\n",
        "        'medieval', 'colonial', 'independence', 'treaty', 'conquest', 'reign',\n",
        "        'founded', 'abolished', 'established', 'era', 'period', 'age',\n",
        "        'BC', 'AD', 'BCE', 'CE', 'year', 'born', 'died', 'ruled'\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in history_keywords)\n",
        "\n",
        "def create_history_qa_dataset(num_samples=5000):\n",
        "    \"\"\"\n",
        "    Create conversational Q&A pairs focused on history topics\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Creating history Q&A dataset...\")\n",
        "\n",
        "    qa_pairs = []\n",
        "\n",
        "    # Question templates for history\n",
        "    history_questions = [\n",
        "        \"Tell me about {}\",\n",
        "        \"What happened during {}?\",\n",
        "        \"Explain the history of {}\",\n",
        "        \"What do you know about {}?\",\n",
        "        \"Can you describe {}?\",\n",
        "        \"Give me information about {}\",\n",
        "        \"What was {}?\",\n",
        "        \"Who was involved in {}?\",\n",
        "    ]\n",
        "\n",
        "    # Follow-up style questions\n",
        "    follow_up_questions = [\n",
        "        \"Can you explain this historical event?\",\n",
        "        \"Tell me more about this period in history.\",\n",
        "        \"What happened here?\",\n",
        "        \"Explain this to me.\",\n",
        "        \"What's the significance of this?\",\n",
        "        \"Can you summarize this historical information?\",\n",
        "        \"What are the key facts about this?\",\n",
        "    ]\n",
        "\n",
        "    processed = 0\n",
        "\n",
        "    for example in wiki_data.select(range(min(len(wiki_data), 20000))):\n",
        "        source = example['source']\n",
        "        target = example['target']\n",
        "\n",
        "        # Skip non-history content and short texts\n",
        "        if not source or not target or len(source) < 50:\n",
        "            continue\n",
        "\n",
        "        if not extract_history_content(source):\n",
        "            continue\n",
        "\n",
        "        processed += 1\n",
        "\n",
        "        # Extract topic (first 50 chars as context)\n",
        "        topic_snippet = source[:50].strip()\n",
        "        if len(topic_snippet) > 45:\n",
        "            topic_snippet = topic_snippet[:45] + \"...\"\n",
        "\n",
        "        # Question with topic context (2 variations)\n",
        "        for template in random.sample(history_questions, 2):\n",
        "            qa_pairs.append({\n",
        "                \"input\": template.format(topic_snippet),\n",
        "                \"output\": target\n",
        "            })\n",
        "\n",
        "        # Follow-up questions with context (2 variations)\n",
        "        for question in random.sample(follow_up_questions, 2):\n",
        "            qa_pairs.append({\n",
        "                \"input\": f\"Context: {source[:200]}\\n\\nQuestion: {question}\",\n",
        "                \"output\": target\n",
        "            })\n",
        "\n",
        "        # Direct explanation request\n",
        "        qa_pairs.append({\n",
        "            \"input\": f\"Explain this historical text: {source[:300]}\",\n",
        "            \"output\": target\n",
        "        })\n",
        "\n",
        "        if processed % 500 == 0:\n",
        "            print(f\"   Processed {processed} history articles, created {len(qa_pairs)} Q&A pairs...\")\n",
        "\n",
        "        if processed >= num_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"‚úÖ Created {len(qa_pairs)} history Q&A pairs from {processed} articles\")\n",
        "    return qa_pairs\n",
        "\n",
        "# Create the dataset\n",
        "history_qa = create_history_qa_dataset(num_samples=5000)\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"history_qa_dataset.json\", \"w\") as f:\n",
        "    json.dump(history_qa, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Dataset saved to 'history_qa_dataset.json'\")\n",
        "print(f\"üìä Total Q&A pairs: {len(history_qa)}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nüìù Sample Q&A pairs:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Q: {history_qa[i]['input'][:150]}...\")\n",
        "    print(f\"A: {history_qa[i]['output'][:150]}...\")\n"
      ],
      "metadata": {
        "id": "4JhakFBdr6WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained Phi-3-mini model with 4-bit quantization for memory-efficient training."
      ],
      "metadata": {
        "id": "r0PjknobsWUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "print(f\"\\nüì¶ Loading model: {model_name}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "RlZzu9HJsHrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Converts raw Q&A pairs into Phi-3's specific chat template format required for training."
      ],
      "metadata": {
        "id": "t2l_yTFesfID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "def format_prompt(example):\n",
        "    \"\"\"Format as instruction-response pairs\"\"\"\n",
        "    return f\"\"\"<|user|>\n",
        "{example['input']}<|end|>\n",
        "<|assistant|>\n",
        "{example['output']}<|end|>\"\"\"\n",
        "\n",
        "formatted_data = [format_prompt(item) for item in history_qa]\n",
        "dataset = Dataset.from_dict({\"text\": formatted_data})\n",
        "\n",
        "print(f\"‚úÖ Dataset formatted: {len(dataset)} training examples\")\n",
        "\n",
        "# Split into train and validation\n",
        "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"   Train: {len(train_dataset)}\")\n",
        "print(f\"   Validation: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "id": "v0uxSZlgsl9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding LoRA Adapters"
      ],
      "metadata": {
        "id": "-OZSvE-2suH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=32,  # LoRA rank (reduced for faster training)\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=64,  # LoRA scaling factor (2x rank)\n",
        "    lora_dropout=0,  # No dropout for optimization\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA adapters added\")"
      ],
      "metadata": {
        "id": "7zGYpBQHsr9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up training arguments and training the model"
      ],
      "metadata": {
        "id": "Ntqf8FuUs1Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,  # Increased from 2\n",
        "        per_device_eval_batch_size=8,\n",
        "        gradient_accumulation_steps=4,  # Effective batch size = 16\n",
        "        warmup_steps=50,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=3407,\n",
        "        output_dir=\"history_chatbot_outputs\",\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=\"none\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer configured\")\n",
        "\n",
        "print(\"\\nüöÄ Starting training...\")\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "elapsed_time = (time.time() - start_time) / 60\n",
        "print(f\"\\nüéâ Training complete in {elapsed_time:.1f} minutes!\")\n"
      ],
      "metadata": {
        "id": "GU7Vggg1s5fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test functionality of the modal"
      ],
      "metadata": {
        "id": "sUpyi07ls87L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)  # Enable faster inference\n",
        "\n",
        "print(\"\\nüß™ Testing the model...\\n\")\n",
        "\n",
        "test_questions = [\n",
        "    \"Tell me about the Roman Empire\",\n",
        "    \"What happened during World War II?\",\n",
        "    \"Explain the French Revolution\",\n",
        "    \"Who was Alexander the Great?\",\n",
        "    \"What was the Renaissance period?\",\n",
        "    \"Tell me about ancient Egypt\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs,\n",
        "        max_new_tokens=256,\n",
        "        use_cache=True,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.1,\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Extract assistant's response\n",
        "    if \"<|assistant|>\" in response:\n",
        "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {response}\\n\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "Ahewt83_s8gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interactive Chat Interface"
      ],
      "metadata": {
        "id": "5p9KvobOtDG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ü§ñ HISTORY CHATBOT - Interactive Mode\")\n",
        "print(\"=\"*80)\n",
        "print(\"Commands: 'quit' to exit\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
        "            print(\"üëã Goodbye!\")\n",
        "            break\n",
        "\n",
        "        if not user_input:\n",
        "            continue\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=256,\n",
        "            use_cache=True,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "\n",
        "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Extract assistant's response\n",
        "        if \"<|assistant|>\" in response:\n",
        "            response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "        print(f\"Bot: {response}\\n\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüëã Goodbye!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "75z5FB2AtGIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43483f78"
      },
      "source": [
        "def calculate_metrics(model, tokenizer, test_questions):\n",
        "    total_response_length = 0\n",
        "    num_questions = len(test_questions)\n",
        "\n",
        "    for question in test_questions:\n",
        "        messages = [{\"role\": \"user\", \"content\": question}]\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=256,\n",
        "            use_cache=True,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "        response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "        if \"<|assistant|>\" in response:\n",
        "            response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "        total_response_length += len(response)\n",
        "\n",
        "    avg_response_length = total_response_length / num_questions if num_questions > 0 else 0\n",
        "    return {\"average_response_length\": avg_response_length}\n",
        "\n",
        "if 'model' in locals() and 'tokenizer' in locals() and 'test_questions' in locals():\n",
        "    metrics = calculate_metrics(model, tokenizer, test_questions)\n",
        "    print(f\"   Average Response Length: {metrics['average_response_length']:.2f}\")\n",
        "else:\n",
        "    print(\"   Model, tokenizer, or test_questions not found. Please run previous cells.\")\n",
        "\n",
        "print(\"‚úÖ Performance evaluation complete\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model in GGUF format"
      ],
      "metadata": {
        "id": "nI2ptivWtKab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüíæ Saving model...\")\n",
        "\n",
        "# Save as GGUF for easy deployment\n",
        "model.save_pretrained_gguf(\"history_chatbot_gguf\", tokenizer, quantization_method=\"q4_k_m\")\n",
        "print(\"‚úÖ Model saved in GGUF format\")\n",
        "\n",
        "# Save full model\n",
        "model.save_pretrained(\"history_chatbot_model\")\n",
        "tokenizer.save_pretrained(\"history_chatbot_model\")\n",
        "print(\"‚úÖ Model saved in HuggingFace format\")"
      ],
      "metadata": {
        "id": "anpyuSH_tJTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}